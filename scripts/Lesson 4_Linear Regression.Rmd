---
title: "Linear Regression"
author: "Conor Fair^[Statistical consultant, cfair13@uga.edu]"
date: "2025-10-2"
output:
  html_document:                           # Options for HTML output
    theme: flatly                           # Sets a Bootstrap theme for HTML
    toc: true                               # Include a table of contents
    toc_float:                              # Floating TOC options
      collapsed: false                      # Show all TOC sections expanded initially
      smooth_scroll: true                   # Smooth scrolling to sections
    number_sections: true                   # Number sections automatically
    fig_caption: true                       # Automatically add figure captions
    df_print: kable                         # Print data frames nicely using knitr::kable
    code_folding: show                      # Allow code chunks to be hidden or shown interactively
    highlight: tango                        # Syntax highlighting style for code
  word_document:                            # Options for Word output
    fig_caption: true                       # Include figure captions
  pdf_document:                             # Options for PDF output
    fig_caption: true                       # Include figure captions
    toc: true                               # Include a table of contents
    number_sections: true                   # Automatically number sections
fontsize: 11pt                              # Sets the base font size in PDF output
geometry: margin=1in                         # Sets page margins for PDF
editor_options: 
  chunk_output_type: console                 # Show code chunk output in the console by default
  markdown: 
    wrap: 72                                # Wrap lines at 72 characters for readability in Markdown view
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r Read Libraries, include=FALSE}
# Necessary Libraries
# Will load packages you have not yet installed - can take more time
pacman::p_load(agridat, tidyverse, readr, here, # data import and handling
               conflicted, # handling function conflicts
               emmeans, multcomp, multcompView, # adjusted mean comparisons
               lme4, #linear modeling
               ggpp, desplot, gridExtra, ggfortify, # plots
               lmtest, stringr, performance, DHARMa) # others

# conflicts: identical function names from different packages
conflict_prefer("filter", "dplyr")
conflict_prefer("select", "dplyr")
conflict_prefer("summarize", "dplyr")
```

# Linear Regression
Linear regression is a means to test a relationship between independent variables(s) (i.e., predictor(s), explanatory variable(s)) and one dependent variable (response variable). Depending on the conditions of the independent variable(s) you may be testing a model that could be called a linear regression, ANOVA, or ANCOVA. We will discuss these differences and similarities. These models also have specific assumptions that must be met to produce estimates of the relationship that are not biased and useful in creating predictions and interpreting the results.  

## One-Way ANOVA
The ANOVA model is a unique class of linear models where the response variable is quantitative (continuous) and the predictors are one or more nominal (categorical) variables. The term ANOVA comes from Analysis Of Variance, which is a method for separating the variance of a group of observations, which was invented by Ronald Fisher (over a century ago).  

ANOVA is further categorized by the number of nominal variables included in the analysis. A one-way ANOVA has a single predictor. The following example shows the important sequential steps that should be used to complete a one-way ANOVA.

### Review the Experimental Design
By reviewing the data and its characteristics you can confirm the appropriate analysis and understand the context of the hypothesis. This will also be helpful when considering how to visualize the data to help interpret the results.  

This example is a pot-experiment to compare weed control efficacy of two herbicides used alone and in mixture. A control was also added as a reference. The four treatments are 1) Metribuzin, 2) Rimsulfuron, 3) Metribuzin+Rimsulfuron, and 4) Untreated Control. 

Sixteen uniform pots were prepared and sown with *Solanum nigrum*. When the plants reached the 4-true-leaves stage they were randomly sprayed with one of the above herbicide solutions following a completely randomized design (will be covered in more detail during the Experimental Design lecture) with four replicates. Three weeks after the treatment, the plants in each pot were harvested and weighed. The theory dictates that the lower the weight the higher the efficacy of herbicides. Our hypothesis was that there was a difference in the response (i.e., weight) among the different treatments. Identifying a treatment that produces the lowest weight would suggest better control of weed growth.

### Importing the Data
When organizing data (commonly in an excel file) note that the dataset is in a "tidy" format, with one row per observation and one column per variable. This is known as the long format and is what most functions expect to be used in R. Other data formats might be useful for some visualizations, but the data can be transformed into other formats when necessary.

```{r Importing One-Way ANOVA Data}
One_Way_Anova <- read_csv(here("data", "One_Way_ANOVA.csv"))
```

### Data Description
The summary function of the data object produces summary statistics of each variable in the dataset. We can also use the tapply function to produce the treatment means and standard deviations for each treatment level. This can also be done using the dplyr function summarize. We get a glimpse at the treatment averages and the variation in standard deviation between treatments. We observed differences in the sample means, but our goal is to determine differences in the population means.

```{r One-Way ANOVA Data Description}
summary(One_Way_Anova)
treatMeans <- tapply(One_Way_Anova$Weight, One_Way_Anova$Treat, mean)
SDs <- tapply(One_Way_Anova$Weight, One_Way_Anova$Treat, sd)
descrit <- data.frame(treatMeans, SDs)
descrit
```

### Fitting ANOVA Models with R
Fitting a linear model with weight as the response variable and the factor of Treatment as the independent variable. You can either call the variable as a factor within the lm function, or you can convert the variable from a character to a factor. You can use the str function or look at the data object in the Environment tab.

```{r One-Way ANOVA Model}
str(One_Way_Anova)
mod <- lm(Weight ~ factor(Treat), data = One_Way_Anova)

One_Way_Anova$Treat <- factor(One_Way_Anova$Treat)
str(One_Way_Anova)
mod <- lm(Weight ~ Treat, data = One_Way_Anova)
```

Following the linear model being fit, you can call the parameter estimates using the summary function. You can see the call which shows the terms included in the model, the summary statistics of the residuals, a table of coefficients, and some other statistical results. This summary output is less useful for ANOVA, but we will revisit this output for linear regression.

```{r One-Way ANOVA Model Results}
summary(mod)
```

After fitting a model the values you are most interested in reviewing are the fitted values and the residuals, which can be extracted using the fitted() and residuals() functions. The fitted values are the same as the means calculated above. The residuals are the deviation each observation is away from the mean value, and will be used for later steps.

```{r One-Way ANOVA Residuals}
expected <- fitted(mod) #Not too useful but included here to show how to extract these values
epsilon <- residuals(mod)
```

The ANOVA table is commonly obtained using one of two methods from either the stats or car package. Certain model types we use in future more complicated models may prefer one over the other. There may be some guidance we can provide, but other situations may require some troubleshooting. It will be helpful to remember these options.

```{r One-Way ANOVA Table}
stats::anova(mod)
car::Anova(mod)
```

This is the type of output that is more often reported for ANOVA models. The interpretation of the F statistic and the p value follow commonly adopted thresholds where a p value < 0.05 is considered significant. The norm is to report the numerator and denominator degrees of freedom, the F value, and the p value (e.g., F(3,12)=23.663, p value < 0.0001). This says that the F-statistic from this model is 23.663 is larger than the critical value from the F-statistic table with numerator degrees of freedom 3 and denominator degrees of freedom 12. This test only tells us that there is a significant difference among the levels of the categorical variable. It doesn't say which levels are significantly different.

### Assumptions
The results of the model are only valid when the assumptions for linear models are met. These are sometimes called the Gauss-Markov Assumptions or BLUE (Best Linear Unbiased Estimator). These assumptions will be the same for ANOVA, ANCOVA, and linear regression.

Some of the assumptions have more to deal with how the data are collected and the experiment is designed - variation in the independent variables, random sampling, and linearity in parameters. The other assumptions are assessed after the model is tested - normally distributed errors (only impacts p-values, and CIs not parameter estimates), and homoskedasticity. There are other assumptions that are more theoretical in nature, and wont be discussed here.  

The assessment of the residuals of the linear model can either be inspected visually in a graph, or by objective, formal hypotheses. The graphical methods are powerful in detecting strong deviations form the basic assumptions, and the formal hypotheses can be used when there are less clear answers.  

To assess the normality of errors (residuals) we often use a QQ (quantile-quantile)-plot. The standardized residuals are plotted against the respective percentiles of a normal distribution. This can either be achieved using the qqnorm and qqline functions with the residuals - we called epsilon earlier, or using the ggplot function with the rstandard(mod) code. Make sure to run the qqnorm and qqline lines together.

```{r One-Way ANOVA QQ Plot}
qqnorm(epsilon)
qqline(epsilon)

ggplot() +
  geom_qq(aes(sample=rstandard(mod))) +
  geom_abline(color="red") +
  theme_classic()
```

The goal is to observe the points fall along the line as close as possible. Some deviation from the line is normal, but look out for strong evidence of a skewed distribution where the tails are further from the line. 

To assess homoskedasticity we look at the plot of the residuals against the fitted values. If the assumption of no systematic error and homogeneous errors is met, then the residuals should be randomly scattered without any visible systematic patterns. We can look for potential outliers, funnel-like patter suggesting heteroscedasticity, or a positive/negative trend in the residuals over the fitted values.

```{r One-Way ANOVA Residual Plot}
plot(residuals(mod) ~ fitted(mod))
abline(h = 0)
```

Alternatively you can use the autoplot function from the ggfortify package to look at a comprehensive output and review the assumptions.

```{r One-Way ANOVA Autoplot}
autoplot(mod)
```

A formal hypothesis used to test the linear model assumptions for normally distributed residuals is the Shapiro-Wilks test on the residuals of the model. The null hypothesis is that the residuals are normally distributed. A significant result p value < 0.05 indicates the residuals are not normally distributed.

```{r One-Way ANOVA Shapiro Test}
shapiro.test(epsilon)
```

A formal hypothesis used to test the linear model assumptions for homogeneity of variance is the Levene's test on the independent variables in the model. The null hypothesis is that the variance across all samples are equal. A significant result p value < 0.05 indicates there is one sample with a different variance. This is a useful test when you have a categorical independent variable where the residuals vs fitted plot would be more useful for a model with only continuous variables.

```{r One-Way ANOVA Levene Test}
car::leveneTest(Weight ~ Treat, One_Way_Anova, center = mean)
```

### Expected Marginal Means
The expected marginal means will produce the arithmetic means when the experiment is balanced (same number of replicates within each group), but the means will be different when the experiment is unbalanced. We will use the emmeans() function to produce the expected marginal means (also known as least squares means). 

```{r One-Way ANOVA Emmeans}
mu_j <- mod %>%
  emmeans( ~ Treat)
mu_j
```

### Multiple Comparisons
Following the evidence to support the linear model meets the assumptions, further analysis can be completed to produce unbiased estimates. The rejection of the null hypothesis that all treatment groups are equal suggests that there is at least one difference. The process to determine which treatment groups are significantly different from the other is known as post-hoc multiple comparisons tests.

Two general approaches for multiple comparisons tests include linear contrasts and pairwise comparisons. 

Linear contrasts are a linear combination of means (model parameters) that add up to zero. One example from the herbicide treatments would be to compare the three herbicide treatments to the control 1/3+1/3+1/3-1=0 Each level of the treatment variable are given a constant that informs what levels are being compared. Here the average of the three treatments are being compared to the control. You can test multiple types of contrasts, but the constants have to sum to zero - this is known as orthogonal contrasts.

```{r One-Way ANOVA Contrasts}
cont1 <- c(1/3, 1/3, 1/3, -1) #the average of the first three levels compared to the fourth
cont2 <- c(1/2, -1, 1/2, 0) #the average of the first and third level compared to the second
cont3 <- c(0, -1, 1, 0) #the second level compared to the third
cont4 <- c(1, -1, 0, 0) #the first level compared to the second
contrasts <- list(cont1, cont2, cont3, cont4)
contrast(mu_j, method = contrasts, adjust = "none") #emmeans from expected marginal means
```

The pairwise comparisons are more commonly used (sometimes overused) and follow two general approaches: 1) all pairwise comparisons, and comparisons with a control (Dunnett's method). All pairwise comparisons will yield a large number of contrasts, and some might not be of primary interest based on the hypothesis. Carefully consider these steps within the context of your data and hypotheses. Furthermore, the "dunnett" method defaults to comparing each level to the first level in alphabetical order, which may not be the control level. You can call the levels of the treatment variable from the dataset object and find that the fourth level is the control.

```{r One-Way ANOVA MCP}
contrast(mu_j, adjust = "none", method = "pairwise")
levels(One_Way_Anova$Treat)
contrast(mu_j, adjust = "none", method = "dunnett", ref = 4)
```

With a large number of contrasts it can be impractical to report the effect size of all comparisons. The cld (compact letter display) function helps to visualize the differences between the treatment levels. You can produce either lowercase letters (letters) or uppercase letters (LETTERS). You can also have the letters follow increasing or decreasing emmean values. Be mindful that the letter display doesn't inform anything about the direction and size of the differences, which is it's chief criticism in their use to present experimental results. This criticism can be alleviated when the letters are showed alongside the raw data.

```{r One-Way ANOVA CLD}
mu_j <- mod %>%
  emmeans(specs = "Treat") %>%
  cld(adjust = "none", Letters = letters)
mu_j

mu_j_reversed <- mod %>%
  emmeans(specs = "Treat") %>%
  cld(adjust = "none", Letters = letters, reversed = TRUE)
mu_j_reversed
```

In experiments that have a high number of contrasts or simultaneously tested hypotheses, there is the potential for an increased type I error. This is known as the multiplicity problem. The adjust="none" code above produces the results of the multiple comparisons without and adjustment to the p values and is known as Fisher's Least Significant Difference (LSD). This is the least conservative approach to multiple comparisons given that there is no attempt to correct for the increased type I error. This approach has been aruged as appropriate when the p value of an F-test is highly significant, but this is up for debate.

Other approaches make adjustments to the p values and the confidence intervals of the pairwise comparisons. The Sidak method is one approach that can be used with the following argument - adjust="sidak".

```{r One-Way ANOVA Sidak}
mu_j <- mod %>%
  emmeans(specs = "Treat") %>%
  cld(adjust = "sidak", Letters = letters)
mu_j
```

Another example is the Bonferroni correction is simpler adjustment procedure and more commonly used. 

```{r One-Way ANOVA Bonferroni}
mu_j <- mod %>%
  emmeans(specs = "Treat") %>%
  cld(adjust = "bonferroni", Letters = letters)
mu_j
```

The last multiplicity adjustment we will discuss is the option I most commonly see: Tukey's HSD. This option is the default method - we just remove the adjust argument. All three are seen as conservative adjustments. You can also see the individual comparisons by including details=TRUE option.

```{r One-Way ANOVA Tukey}
mu_j<-mod %>%
  emmeans(specs = "Treat") %>%
  cld(Letters = letters, details = F)
mu_j
```

We can use the results of the multiple comparisons test to help answer our hypotheses.

### Data Visualization

```{r One-Way ANOVA Final Figure}
mu_j <- mu_j %>%
  as_tibble()

Plot_means_tukey <- ggplot() +
  geom_point(data = One_Way_Anova, aes(y = Weight, x = Treat), position = position_jitter(width = 0.1)) + #dots representing the raw data
  geom_boxplot(data = One_Way_Anova, aes(y = Weight, x = Treat), position = position_nudge(x = -0.25), width = 0.25, outlier.shape = NA) + #boxplot
  geom_point(data = mu_j, aes(y = emmean, x = Treat), position = position_nudge(x = 0.15), size = 2, color = "red") + # red dots representing the adjusted means
  geom_errorbar(data = mu_j, aes(ymin = lower.CL, ymax = upper.CL, x = Treat), position = position_nudge(x = 0.15), color = "red", width = 0.1) + # red error bars representing the confidence limits of the adjusted means
  geom_text(data=mu_j,aes(y=emmean,x=Treat,label=str_trim(.group)),position=position_nudge(x=0.25),color="black",angle=0)+ # red letters 
  labs(y="Weed Weight (g)",x="Treatment")+
  theme_classic()
Plot_means_tukey
```

All treatments are better than the control (i.e., unweeded). Mixture_378 is a better treatment than Rimsulfuron_30, and Metribuzin_348 is no better than either of the treatments.

## Two-Way ANOVA 

When you have multiple independent variables in the linear model that are all categorical this is known as a two-way ANOVA. The following steps demonstrate how to perform an analysis of a two-way ANOVA.

### Review the Experimental Design

Review the data and its characteristics you can confirm the appropriate analysis and understand the context of the hypothesis. This will also be helpful when considering how to visualize the data to help interpret the results.

These data show the body mass in grams among different penguin species on different islands and between sexes of penguins. There are other variables recorded that could be analyzed, but we will be focusing on the species, sex, and body mass (g) of the penguins.

### Importing the Data

Remember the "tidy" format...

```{r Importing Two-Way ANOVA Data}
Two_Way_Anova <- read_csv(here("data", "Two_Way_ANOVA.csv"))

Two_Way_Anova <- subset(Two_Way_Anova, !is.na(sex)) # removes NAs within the sex variable
```

### Data Description

The full dataset contains eight variables for 333 penguins (without NAs). Make sure to check the state/type of each variable in the dataframe to make sure the categorical variables are considered as Factors. You can either look in the Environment tab or use the str() function.

```{r Two-Way ANOVA Data Description}
summary(Two_Way_Anova)
str(Two_Way_Anova)

Two_Way_Anova$species <- factor(Two_Way_Anova$species)
Two_Way_Anova$sex <- factor(Two_Way_Anova$sex)
str(Two_Way_Anova)
```

### Fitting ANOVA Models with R

Fitting a linear model with body_mass_g as the response variable and the factors of species and sex as the independent categorical variables. 

We are interested in determining the following 
1) The variation in body mass among the different species of penguins. 
2) The variation in body mass among the different sexes of penguins. 
3) The variation in body mass among the different species of penguins is different for females and males (interaction). 

Hypothesis tests are as follows  
Main effect of sex on body mass:  
  H0: mean body mass is equal between females and males  
  H1: mean body mass is different between females and males
Main effect of species on body mass  
  H0: mean body mass is equal between all three species
  H1: mean body mass is different for at least one species
Interaction between sex and species:  
  H0: there is no interaction between sex and species, meaning that the relationship between species and body mass is the same for females and males  
  H1: there is an interaction between sex and species, meaning that the relationship between species and body mass is different for females than for males

```{r Two-Way ANOVA Model}
mod <- lm(body_mass_g ~ species * sex, data = Two_Way_Anova)
```

Following the linear model being fit, we call the parameter estimates using the summary function.

```{r Two-Way ANOVA Model Results}
summary(mod)
```

And we extract the residuals values.

```{r Two-Way ANOVA Residuals}
epsilon <- residuals(mod)
```

In this instance we are testing both species and sex and their interaction so the type III sums of squares is most appropriate. Type I sums of squares tests each factor sequentially, and the type II sums of squares is inappropriate when there is an interaction term.

This [link](https://md.psych.bio.uni-goettingen.de/mv/unit/lm_cat/lm_cat_unbal_ss_explained.html) can provide more details for your reference.

```{r Two-Way ANOVA Table}
stats::anova(mod) #Presents type I sums of squares
car::Anova(mod) #Presents type II sums of squares
car::Anova(mod, type = "III")
```

Remember that this test only tells us that there is a significant difference among the levels of the categorical variable. It doesn't say which levels are significantly different.

### Assumptions
To assess the normality of errors (residuals) we use a QQ (quantile-quantile)-plot. The standardized residuals are plotted against the respective percentiles of a normal distribution. 

```{r Two-Way ANOVA QQ-Plot}
qqnorm(epsilon)
qqline(epsilon)
```

To assess homoskedasticity we look at the plot of the residuals against the fitted values. 

```{r Two-Way ANOVA Residuals Plot}
plot(residuals(mod) ~ fitted(mod))
abline(h = 0)

autoplot(mod)
```

Then the Shapiro-Wilks test on the residuals of the model. 

```{r Two-Way ANOVA Shapiro Test}
shapiro.test(epsilon)
```

Finally the Levene's test on the independent variables in the model. 

```{r Two-Way ANOVA Levene Test}
car::leveneTest(mod ~ species * sex, data = Two_Way_Anova)
```

Results of the above figures and tests indicate that model assumptions have been met.

### Expected Marginal Means

We can call for the emmeans for both main effects and the interaction.

```{r Two-Way Emmeans}
mu_species <- mod %>%
  emmeans(~ species) %>%
  as_tibble() %>% #Converts the object to a tibble
  arrange(emmean) #Arranges the emmean values as ascending values
mu_species

mu_sex <- mod %>%
  emmeans(~ sex) %>%
  as_tibble() %>%
  arrange(emmean)
mu_sex

group_by(Two_Way_Anova, species, sex) %>%
  summarise(mean = round(mean(body_mass_g, na.rm = TRUE)),
            sd = round(sd(body_mass_g, na.rm = TRUE)))
```

### Multiple Comparisons

Following the evidence to support the linear model meets the assumptions, further analysis can be completed to produce unbiased estimates. The rejection of the null hypothesis that body mass is the same across the species and sexes suggests there is a difference among the levels of one variable across the other variable. We move onto the post-hoc multiple comparisons tests.

When you have a significant interaction effect you shouldn't analyze the main effects independently since their interpretation is dependent on the other effect. There is another option for multiple comparisons tests when you have multiple factors and interactions. You can either test all pairwise comparisons by using a colon between the two terms, or you can test pairwise comparisons of one effect within the levels of another effect by using a pipe between the two terms. We will show both methods as an example.

```{r Two-Way ANOVA Tukey}
all_means_tukey <- mod %>%
  emmeans(~ species : sex) %>%
  cld(Letters = letters)
all_means_tukey

within_means_tukey <- mod %>%
  emmeans(~ species | sex) %>%
  cld(Letters = letters)
within_means_tukey
```

### Data Visualization

```{r Two-Way ANOVA Final Figure}
all_means_tukey <- all_means_tukey %>%
  as_tibble() #convert object to tibble so that ggplot can read the details

Plot_all_means_tukey <- ggplot() +
  geom_point(data = Two_Way_Anova, aes(y = body_mass_g, x = species, color = sex), position = position_dodgenudge(width = 0.9, x = 0)) + #dots representing the raw data
  geom_boxplot(data = Two_Way_Anova, aes(y = body_mass_g, x = species, color = sex), width = 0.25, outlier.shape = NA, position = position_dodgenudge(width = 0.5, x = 0)) + # boxplot
  geom_point(data = all_means_tukey, aes(y = emmean, x = species, group = sex), position = position_dodgenudge(width = 0.5, x = 0), size = 2, color = "red") + # red dots representing the adjusted means
  geom_errorbar(data = all_means_tukey, aes(ymin = lower.CL, ymax = upper.CL, x = species, group = sex), position = position_dodgenudge(width = 0.5, x = 0),color = "red", width = 0.1) + # red error bars representing the confidence limits of the adjusted means
  geom_text(data = all_means_tukey, aes(y = emmean, x = species, group = sex, label = str_trim(.group)), color = "black", position = position_dodgenudge(width = 1.2, x = -0.025), hjust = 0, angle = 0) + # red letters 
  labs(y = "Body Mass (g)", x = "Species") +
  theme_classic()
Plot_all_means_tukey

within_means_tukey <- within_means_tukey %>%
  as_tibble()

Plot_within_means_tukey <- ggplot() +
  facet_wrap(~ sex, labeller = label_both) + #facet per sex level
  geom_point(data = Two_Way_Anova, aes(y = body_mass_g, x = species, color = species)) + # dots representing the raw data
  geom_boxplot(data = Two_Way_Anova, aes(y = body_mass_g, x = species, color = species), width = 0.25, outlier.shape = NA, position = position_nudge(x = 0.2)) + # boxplot
  geom_point(data = within_means_tukey, aes(y = emmean, x = species), color = "red", position = position_nudge(x = 0.2)) + # red dots representing the adjusted means
  geom_errorbar(data = within_means_tukey, aes(ymin = lower.CL, ymax = upper.CL, x = species), color = "red", width = 0.1, position = position_nudge(x = 0.2)) + # red error bars representing the confidence limits of the adjusted means
  geom_text(data = within_means_tukey, aes(y = emmean, x = species, label = str_trim(.group)), color = "black", position = position_nudge(x = 0.4), hjust = 0) + # red letters 
  labs(y = "Body Mass (g)", x = "Species") +
  theme_classic() # clearer plot format 
Plot_within_means_tukey
```

## Simple Linear Regression

When you have one or more independent variables in the linear model that are all continuous that is known as linear regression. Simple linear regression has one independent variable, and multiple linear regression has multiple independent variables.

### Review the Experimental Design

Data from (Kniss et al. 2012) shows the sugar yield of sugar beet in response to volunteer corn density. The response variable is sucrose production in pounds per acre (LbsSucA), and the independent variable is volunteer corn density in plants per foot of row.

### Importing the Data

```{r Importing Regression Data}
Simple_Linear_Regression <- read_csv(here("data", "Simple_Linear_Regression.csv"))
```

### Fitting Linear Model with R

```{r Regression Model}
mod <- lm(LbsSucA ~ Density, data = Simple_Linear_Regression)
```

Following the linear model being fit, you can call the parameter estimates using the summary function.

```{r Regression Model Results}
summary(mod)
```

The resulting output provides estimates for the intercept (8677.6) and slope (-20644.7) of the regression line. The regression for this example would be Y (Pounds Sucrose/Acre)=8677.6-20644.7*X (Density). The interpretation of these results says that for every one unit change in the Volunteer corn density (plants/ft row) results in a 20,644.7 unit decrease in the yield. The maximum volunteer corn density used in the study was only 0.15 plants per foot of sugar beet row, so a one unit change in the density is beyond the scale of the data used in the study. We can still use the above equation to calculate the estimated pounds sucrose per acre for a fraction of a change in the density.

The ANOVA table can be used but provides little information here. Furthermore, notice that the results for the effect of density do not change between the methods used - there is only one effect and no interaction.

```{r Regression ANOVA Table}
stats::anova(mod) #Presents type I sums of squares
car::Anova(mod) #Presents type II sums of squares
car::Anova(mod, type = "III")
```

### Assumptions

To assess the normality of errors (residuals) we use the QQ (quantile-quantile)-plot again.

```{r Regression QQ-Plot}
qqnorm(mod$residuals)
qqline(mod$residuals)

ggplot() +
  geom_qq(aes(sample=rstandard(mod))) +
  geom_abline(color = "red") +
  theme_classic()
```

To assess homoskedasticity we look at the plot of the residuals against the fitted values.

```{r Regression Residuals}
plot(residuals(mod) ~ fitted(mod))
abline(h = 0)

autoplot(mod)
```

Then the Shapiro-Wilks test on the residuals of the model.

```{r Regression Shapiro Test}
shapiro.test(mod$residuals)
```

No need to do a levene test since there is no categorical variable included in the model. The Breusch-Pagan test can be used instead.

```{r Regression BP test}
bptest(mod)
```


### Data Visualization

This is a simple plot for the simple linear regression model above. There will be more complicated figures and syntax required with more complicated models.

```{r Regression Final Figure}
par(mfrow = c(1, 1), mar = c(3.2, 3.2, 2, 0.5), mgp = c(2, 0.7, 0)) #Number of rows of figures, the margins of the figure, and margin line for the axis title labels and line
plot(Simple_Linear_Regression$LbsSucA ~ Simple_Linear_Regression$Density, bty = "l", #Y and X variables for the plot and the "L" box type for the plot area
     ylab = "Sugar yield (lbs/A)", xlab = "Volunteer corn density (plants/ft row)",
     main = "Lingle, WY 2009", ylim = c(0, 10000))
  abline(mod) # Add the regression line
# to add the regression equation into the plot:
  int <- round(summary(mod)$coefficients[[1]]) # get intercept
  sl <- round(summary(mod)$coefficients[[2]]) # get slope
  reg.eq <- paste("Y =", int, sl, "* X") # create text regression equation
  legend("bottomleft",reg.eq, bty = "n")
```

The results from this model focus on the estimate of the slope (density) which has a negative value. This suggests that as density increases, yield will also decrease.

## ANCOVA

When you have multiple independent variables in the linear model where at least one is categorical and at least one is continuous that is known as ANCOVA (analysis of covariance). The following steps demonstrate how to perform an analysis of ANCOVA.

### Review the Experimental Design

By reviewing the data and its characteristics you can confirm the appropriate analysis and understand the context of the hypothesis. This will also be helpful when considering how to visualize the data to help interpret the results.

### Importing the Data

Remember the "tidy" format...

```{r Importing ANCOVA Data}
ANCOVA <- read_csv(here("data", "ANCOVA.csv")) %>%
  mutate(loc = as.factor(loc)) # Convert loc to a factor
```

### Data Description

The dataset contains the results from a onion trial that tested the effect of planting density (plants per square meter) at two locations (Purnong Landing or Virginia).

```{r ANCOVA Data Description}
summary(ANCOVA)
```

### Initial Visualization of the Data

```{r ANCOVA Data Visualization}
ggplot(ANCOVA, aes(x = density, y = yield, color = loc)) +
  geom_point() +
  theme_classic()
```

### Fitting ANOVA Models with R

Fitting a linear model with yield as the response variable and the factors of density (continuous) and location (categorical) as the independent variables. Make sure to check the state/type of each variable in the dataframe to make sure the categorical variable is considered a Factor. You can either look in the Environment tab or use the str() function.

We are interested in determining the following 
1) The variation in yield among the different locations
2) The variation in yield along the gradient of density
3) The variation in yield along the gradient of density is different between two locations (interaction)

Hypothesis tests are as follows  
Main effect of location on yield:  
  H0: mean yield is equal between locations  
  H1: mean yield is different between locations  
Main effect of density on yield:  
  H0: slope of trend line between density and yield is zero  
  H1: slope of trend line between density and yield is not zero  
Interaction between location and density:  
  H0: there is no interaction between location and density, meaning that the relationship between density and yield is the same for both locations  
  H1: there is an interaction between location and density, meaning that the relationship between density and yield is different between locations  

```{r ANCOVA Model}
str(ANCOVA)

mod <- lm(yield ~ density * loc, data = ANCOVA)
```

Following the linear model being fit, you can call the parameter estimates using the summary function.

```{r ANCOVA Model Results}
summary(mod)
```

After fitting a model we extract the residuals.

```{r ANCOVA Residuals}
epsilon <- residuals(mod)
```

The ANOVA table needs the type 3 test for the interaction effect

```{r ANCOVA ANOVA Table}
car::Anova(mod, type = "III")
```

The interaction effect isn't significant, but each main effect is. Remember that this test only tells us that there is a significant difference among the levels of the categorical variable. It doesn't say which levels are significantly different.

### Assumptions

Assess the normality of errors (residuals) using a QQ (quantile-quantile)-plot.

```{r ANCVOA QQ-Plot}
qqnorm(mod$residuals)
qqline(mod$residuals)

ggplot() +
  geom_qq(aes(sample = rstandard(mod))) +
  geom_abline(color = "red") +
  theme_classic()
```

Some deviation from the line is normal, but look out for strong evidence of a skewed distribution where the tails are further from the line.

Then assess homoskedasticity with the plot of the residuals against the fitted values. 

```{r ANCOVA Residuals Plot}
plot(residuals(mod) ~ fitted(mod))
abline(h=0)

autoplot(mod)
```

The formal hypothesis used to test the linear model assumptions for normally distributed residuals is the Shapiro-Wilks test on the residuals of the model. The null hypothesis is that the residuals are normally distributed. A significant result p value < 0.05 indicates the residuals are not normally distributed.

```{r ANCOVA Shapiro Test}
shapiro.test(mod$residuals)
```

In the presence of results suggesting failure to meet one of the linear model assumptions different stabilizing transformation are available to remove potential biases and produce better estimates from the linear model. Logarithmic transformation are often used for counts and proportions. The square root transformation is also used for counts. The arcsin-square root transformation is very common with proportions.

These transformations were necessary prior to the advancements in computing power that allow for generalized linear models (glm) and generalized linear least squares models (gls) that deal with non-normal and heteroscedastic results. Other options we will discuss include nonparametric methods that make few or no assumptions about the distribution of residuals.

Given the results of the residuals plot and the qqplot we will try different transformations to meet the model assumptions.

```{r ANCOVA Transformations}
mod_log <- lm(log(yield) ~ density * loc, data = ANCOVA)
summary(mod_log)

autoplot(mod_log)

mod_sqrt <- lm(sqrt(yield) ~ density * loc, data = ANCOVA)
summary(mod_sqrt)

autoplot(mod_sqrt)
```

Neither of the transformations improve the distribution of the residuals, so we will try fitting a model that includes a polynomial term to address the curved linear relationship with the residuals.

```{r ANCOVA Polynomial Model}
mod_2 <- lm(yield ~ poly(density, 2) * loc, data = ANCOVA)
summary(mod_2)

autoplot(mod_2) #This greatly improves the residuals plot and qqplot

car::Anova(mod_2, type = "III")
```

We find the same results that the interaction effect isn't significant, and we can now move onto the multiple comparisons steps.

### Expected Marginal Means

We will use the emmeans() function to produce the expected marginal means. 

```{r ANCOVA Emmeans}
mu_location <- mod_2 %>%
  emmeans(~ loc) %>% # same as , at = list(density = mean(ANCOVA$density)) - conditional effect
  cld(Letters = letters) %>%
  as_tibble() %>% # Converts the object to a tibble
  arrange(emmean) # Arranges the emmean values as ascending values
mu_location
```

### Multiple Comparisons

We are comparing the slopes of the two trend lines to see if they are significantly different. The results of the ANOVA table indicate that they are not significantly different. We can review the multiple comparisons results to demonstrate how you would show significant results in the event that future examples show a significant interaction effect.

```{r ANCVOA CLD}
#emtrends to compare slopes between categorical variables
mu_density_linear <- mod_2 %>%
  emtrends(pairwise ~ loc, var = "density", at = list(density = mean(ANCOVA$density)), deriv = 1) %>% 
  cld(Letters = letters)
mu_density_linear

mu_density_quadratic <- mod_2 %>%
  emtrends(pairwise ~ loc, var = "density", deriv = 2) %>%
  cld(Letters = letters)
mu_density_quadratic
```

### Data Visualization

We need to produce a trend line for each location. We will use the expand.grid() and predict() functions to produce out of sample predictions that will be the trend line. The expand.grid() function would need to include other terms from the model - not relevant in this example. You would set either the mean or median of the other variables since you are testing the relationship between density and yield for both locations - holding all other variables constant.

```{r ANCOVA Final Figure}
summary(ANCOVA)
new.x <- expand.grid(density = seq(18.78, 184.75, length.out=100),
                   loc = levels(ANCOVA$loc))
head(new.x)
new.y <- predict(mod_2, newdata = new.x, interval = 'confidence')

#Bring new.x and new.y together
addThese <- data.frame(new.x, new.y)
addThese <- rename(addThese, yield = fit)
head(addThese)

ggplot(ANCOVA, aes(x = density, y = yield, color = loc)) +
  geom_point(size = 5) +
  geom_smooth(data = addThese, aes(ymin = lwr, ymax = upr, fill = loc), stat = 'identity') +
  scale_color_manual(values = c(P = "blue", V = "red")) +
  scale_fill_manual(values = c(P = "blue", V = "red")) +
  theme_classic()
```

We see parallel trend lines suggesting no significant difference in the relationship between density and yield among the two locations.

# Practice Example 

We are going to revisit the penguin data to use as practice. We can upload the data again.

```{r Practice Data}
Two_Way_Anova<-read_csv(here("data","Two_Way_ANOVA.csv")) %>%
  mutate(island = as.factor(island),
         year = as.factor(year),
         species = as.factor(species))

Two_Way_Anova <- subset(Two_Way_Anova, !is.na(sex)) #removes NAs within the sex variable
str(Two_Way_Anova)
```

Example 1
Following the guidelines above test the following hypotheses:

1) The variation in body mass is equal among the three different islands
2) The variation in body mass is equal among the three different years
3) The variation in body mass is equal among the three different islands and three different years (interaction)

Think about which type of model you need to use to test these hypotheses. Report the results from your assessments of the linear model assumptions. Use the multiple comparisons results in a final figure to demonstrate how the results support or refute the above hypotheses.

Example 2
Following the guidelines above test the following hypotheses:

1) The change in flipper length results in an increase in body mass
2) The variation in body mass is equal among the three different species
3) The change in flipper length results in the same increase in body mass among the three different species (interaction)

Think about which type of model you need to use to test these hypotheses. Report the results from your assessments of the linear model assumptions. Use the multiple comparisons results in a final figure to demonstrate how the results support or refute the above hypotheses.
